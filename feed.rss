<?xml version="1.0"?>
<rss version="2.0">
  <channel>
    <title>Reproducibility News Feed</title>
    <link>http://reproduciblescience.org/</link>
    <description>A feed that shows recent news about scientific reproducibility efforts.</description>

    <item>
      <title>Replicability or reproducibility? On the replication crisis in computational neuroscience and sharing only relevant detail</title>
      <link>https://link.springer.com/article/10.1007/s10827-018-0702-z</link>
      <pubDate>Thu, 01 Nov 2018 00:00:00 -0000</pubDate>
      <description>
        Replicability and reproducibility of computational models has been somewhat understudied by “the replication movement.” In this paper, we draw on methodological studies into the replicability of psychological experiments and on the mechanistic account of explanation to analyze the functions of model replications and model reproductions in computational neuroscience. We contend that model replicability, or independent researchers' ability to obtain the same output using original code and data, and model reproducibility, or independent researchers' ability to recreate a model without original code, serve different functions and fail for different reasons. This means that measures designed to improve model replicability may not enhance (and, in some cases, may actually damage) model reproducibility. We claim that although both are undesirable, low model reproducibility poses more of a threat to long-term scientific progress than low model replicability. In our opinion, low model reproducibility stems mostly from authors' omitting to provide crucial information in scientific papers and we stress that sharing all computer code and data is not a solution. Reports of computational studies should remain selective and include all and only relevant bits of code.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>To Clean or Not to Clean: Document Preprocessing and Reproducibility</title>
      <link>https://dl.acm.org/citation.cfm?id=3242180</link>
      <pubDate>Thu, 01 Nov 2018 00:00:00 -0000</pubDate>
      <description>
        Web document collections such as WT10G, GOV2, and ClueWeb are widely used for text retrieval experiments. Documents in these collections contain a fair amount of non-content-related markup in the form of tags, hyperlinks, and so on. Published articles that use these corpora generally do not provide specific details about how this markup information is handled during indexing. However, this question turns out to be important: Through experiments, we find that including or excluding metadata in the index can produce significantly different results with standard IR models. More importantly, the effect varies across models and collections. For example, metadata filtering is found to be generally beneficial when using BM25, or language modeling with Dirichlet smoothing, but can significantly reduce retrieval effectiveness if language modeling is used with Jelinek-Mercer smoothing. We also observe that, in general, the performance differences become more noticeable as the amount of metadata in the test collections increase. Given this variability, we believe that the details of document preprocessing are significant from the point of view of reproducibility. In a second set of experiments, we also study the effect of preprocessing on query expansion using RM3. In this case, once again, we find that it is generally better to remove markup before using documents for query expansion.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>The Rule of Two: A Star Wars Edict or a Method of Reproducibility and Quality?</title>
      <link>https://www.preprints.org/manuscript/201810.0357/v1</link>
      <pubDate>Tue, 23 Oct 2018 00:00:00 -0000</pubDate>
      <description>
        In recent years, biomedical research has faced increased scrutiny over issues related to reproducibility and quality in scientific findings(1-3). In response to this scrutiny, funding institutions and journals have implemented top-down policies for grant and manuscript review. While a positive step forward, the long-term merit of these policies is questionable given their emphasis on completing a check-list of items instead of a fundamental re-assessment of how scientific investigation is conducted. Moreover, the top-down style of management used to institute these policies can be argued as being ineffective in engaging the scientific workforce to act upon these issues. To meet current and future biomedical needs, new investigative methods that emphasize collective-thinking, teamwork, shared knowledge and cultivate change from the bottom-up are warranted. Here, a perspective on a new approach to biomedical investigation within the individual laboratory that emphasizes collaboration and quality is discussed.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Editorial:  Revised Guidelines to Enhance the Rigor and Reproducibility of Research Published in American Physiological Society Journals</title>
      <link>https://www.physiology.org/doi/pdf/10.1152/ajpregu.00274.2018</link>
      <pubDate>Tue, 23 Oct 2018 00:00:00 -0000</pubDate>
      <description>
        A challenge in modern research is the common inability to repeat novel findings published in even the most “impact-heavy” journals. In the great majority of instances, this may simply be due to a failure of the published manuscripts to include—and the publisher to require— comprehensive information on experimental design, methods, reagents, or the in vitro and in vivo systems under study. Failure to accurately reproduce all environmental influences on an experiment, particularly those using animals, also contributes to inability to repeat novel findings. The most common reason for failures of reproducibility may well bein the rigor and transparency with which methodology is described by authors. Another reason may be the reluctance by more established investigators to break with traditional methods of data presentation. However, one size does not fit all when it comes to data presentation, particularly because of the wide variety of data formats presented in individual disciplines represented by journals. Thus, some flexibility needs to be allowed. The American Physiological Society (APS) has made available guidelines for transparent reporting that it recommends all authors follow(https://www.physiology.org/author-info.promoting-transparent-reporting) (https://www.physiology.org/author-info.experimental-details-to-report). These are just some of the efforts being made to facilitate the communication of discovery in a transparent manner, which complement what has been a strength of the discipline for many years—the ability of the scientists and scientific literature to self-correct (8).
      </description>

      <category>news article</category>

    </item>

    <item>
      <title>Teaching Computational Reproducibility for Neuroimaging</title>
      <link>https://www.frontiersin.org/articles/10.3389/fnins.2018.00727/full</link>
      <pubDate>Tue, 23 Oct 2018 00:00:00 -0000</pubDate>
      <description>
        We describe a project-based introduction to reproducible and collaborative neuroimaging analysis. Traditional teaching on neuroimaging usually consists of a series of lectures that emphasize the big picture rather than the foundations on which the techniques are based. The lectures are often paired with practical workshops in which students run imaging analyses using the graphical interface of specific neuroimaging software packages. Our experience suggests that this combination leaves the student with a superficial understanding of the underlying ideas, and an informal, inefficient, and inaccurate approach to analysis. To address these problems, we based our course around a substantial open-ended group project. This allowed us to teach: (a) computational tools to ensure computationally reproducible work, such as the Unix command line, structured code, version control, automated testing, and code review and (b) a clear understanding of the statistical techniques used for a basic analysis of a single run in an MR scanner. The emphasis we put on the group project showed the importance of standard computational tools for accuracy, efficiency, and collaboration. The projects were broadly successful in engaging students in working reproducibly on real scientific questions. We propose that a course on this model should be the foundation for future programs in neuroimaging. We believe it will also serve as a model for teaching efficient and reproducible research in other fields of computational science.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Experimental deception: Science, performance, and reproducibility</title>
      <link>https://psyarxiv.com/93p45/</link>
      <pubDate>Tue, 23 Oct 2018 00:00:00 -0000</pubDate>
      <description>
        Experimental deception has not been seriously examined in terms of its impact on reproducible science. I demonstrate, using data from the Open Science Collaboration’s Reproducibility Project (2015), that experiments involving deception have a higher probability of not replicating and have smaller effect sizes compared to experiments that do not have deception procedures. This trend is possibly due to missing information about the context and performance of agents in the studies in which the original effects were generated, leading to either compromised internal validity, or an incomplete specification and control of variables in replication studies. Of special interest are the mechanisms by which deceptions are implemented and how these present challenges for the efficient transmission of critical information from experimenter to participant. I rehearse possible frameworks that might form the basis of a future research program on experimental deception and make some recommendations as to how such a program might be initiated.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>University researchers push for better research methods</title>
      <link>http://www.mndaily.com/article/2018/10/n-university-researchers-push-for-better-research-methods</link>
      <pubDate>Tue, 16 Oct 2018 00:00:00 -0000</pubDate>
      <description>
        Faculty members and graduate students at the University of Minnesota have formed a workshop to hold discussions about reproducibility in research studies. The discussions come during a national movement to replicate research in social science fields, such as psychology. The movement has shown many previous studies are not reliable.  After discussions last spring regarding ways the University can address these research practices, the Minnesota Center for Philosophy of Science designed workshops for faculty and students to discuss ways to develop replicable research methods.
      </description>

      <category>popular media</category>

    </item>

    <item>
      <title>Open Science as Better Gatekeepers for Science and Society: A Perspective from Neurolaw</title>
      <link>https://psyarxiv.com/8dr23/</link>
      <pubDate>Mon, 22 Oct 2018 00:00:00 -0000</pubDate>
      <description>
        Results from cognitive neuroscience have been cited as evidence in courtrooms around the world, and their admissibility has been a challenge for the legal system. Unfortunately, the recent reproducibility crisis in cognitive neuroscience, showing that the published studies in cognitive neuroscience may not be as trustworthy as expected, has made the situation worse. Here we analysed how the irreproducible results in cognitive neuroscience literature could compromise the standards for admissibility of scientific evidence, and pointed out how the open science movement may help to alleviate these problems. We conclude that open science not only benefits the scientific community but also the legal system, and society in a broad sense. Therefore, we suggest both scientists and practitioners follow open science recommendations and uphold the best available standards in order to serve as good gatekeepers in their own fields. Moreover, scientists and practitioners should collaborate closely to maintain an effective functioning of the entire gatekeeping system of the law.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Responding to the growing issue of research reproducibility</title>
      <link>https://avs.scitation.org/doi/pdf/10.1116/1.5049141</link>
      <pubDate>Tue, 16 Oct 2018 00:00:00 -0000</pubDate>
      <description>
        An increasing number of studies, surveys, and editorials highlight experimental and computational reproducibility and replication issues that appear to pervade most areas of modern science. This perspective examines some of the multiple and complex causes of what has been called a "reproducibility crisis," which can impact materials, interface/(bio)interphase, and vacuum sciences. Reproducibility issues are not new to science, but they are now appearing in new forms requiring innovative solutions. Drivers include the increasingly multidiscipline, multimethod nature of much advanced science, increased complexity of the problems and systems being addressed, and the large amounts and multiple types of experimental and computational data being collected and analyzed in many studies. Sustained efforts are needed to address the causes of reproducibility problems that can hinder the rate of scientific progress and lower public and political regard for science. The initial efforts of the American Vacuum Society to raise awareness of a new generation of reproducibility challenges and provide tools to help address them serve as examples of mitigating actions that can be undertaken.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Building a Reproducible Machine Learning Pipeline</title>
      <link>https://arxiv.org/abs/1810.04570v1</link>
      <pubDate>Tue, 16 Oct 2018 00:00:00 -0000</pubDate>
      <description>
        Reproducibility of modeling is a problem that exists for any machine learning practitioner, whether in industry or academia. The consequences of an irreproducible model can include significant financial costs, lost time, and even loss of personal reputation (if results prove unable to be replicated). This paper will first discuss the problems we have encountered while building a variety of machine learning models, and subsequently describe the framework we built to tackle the problem of model reproducibility. The framework is comprised of four main components (data, feature, scoring, and evaluation layers), which are themselves comprised of well defined transformations. This enables us to not only exactly replicate a model, but also to reuse the transformations across different models. As a result, the platform has dramatically increased the speed of both offline and online experimentation while also ensuring model reproducibility.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Reference environments: A universal tool for reproducibility in computational biology</title>
      <link>https://arxiv.org/pdf/1810.03766.pdf</link>
      <pubDate>Mon, 15 Oct 2018 00:00:00 -0000</pubDate>
      <description>
        The drive for reproducibility in the computational sciences has provoked discussion and effort across a broad range of perspectives: technological, legislative/policy, education, and publishing. Discussion on these topics is not new, but the need to adopt standards for reproducibility of claims made based on computational results is now clear to researchers, publishers and policymakers alike. Many technologies exist to support and promote reproduction of computational results: containerisation tools like Docker, literate programming approaches such as Sweave, knitr, iPython or cloud environments like Amazon Web Services. But these technologies are tied to specific programming languages (e.g. Sweave/knitr to R; iPython to Python) or to platforms (e.g. Docker for 64-bit Linux environments only). To date, no single approach is able to span the broad range of technologies and platforms represented in computational biology and biotechnology. To enable reproducibility across computational biology, we demonstrate an approach and provide a set of tools that is suitable for all computational work and is not tied to a particular programming language or platform. We present published examples from a series of papers in different areas of computational biology, spanning the major languages and technologies in the field (Python/R/MATLAB/Fortran/C/Java). Our approach produces a transparent and flexible process for replication and recomputation of results. Ultimately, its most valuable aspect is the decoupling of methods in computational biology from their implementation. Separating the 'how' (method) of a publication from the 'where' (implementation) promotes genuinely open science and benefits the scientific community as a whole.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>The Brazilian Reproducibility Initiative: a systematic assessment of Brazilian biomedical science</title>
      <link>https://osf.io/ahf7t/</link>
      <pubDate>Mon, 08 Oct 2018 00:00:00 -0000</pubDate>
      <description>
        With concerns over research reproducibility on the rise, systematic replications of published science have become an important tool to estimate the replicability of findings in specific areas. Nevertheless, such initiatives are still uncommon in biomedical science, and have never been performed at a national level. The Brazilian Reproducibility Initiative is a multicenter, systematic effort to assess the reproducibility of the country’s biomedical research by replicating between 50 and 100 experiments from Brazilian life sciences articles. The project will focus on a set of common laboratory methods, performing each experiment in multiple institutions across the country, with the reproducibility of published findings analyzed in the light of interlaboratory variability. The results, due in 2021, will allow us not only to estimate the reproducibility of Brazilian biomedical science, but also to investigate if there are aspects of the published literature that can be used to predict it.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Towards Reproducible and Reusable Deep Learning Systems Research Artifacts</title>
      <link>https://openreview.net/pdf?id=Ske-2Gyk9X</link>
      <pubDate>Mon, 08 Oct 2018 00:00:00 -0000</pubDate>
      <description>
        This paper discusses results and insights from the 1st ReQuEST workshop, a collective effort to promote reusability, portability and reproducibility of deep learning research artifacts within the Architecture/PL/Systems communities. ReQuEST (Reproducible Quality-Efficient Systems Tournament) exploits the open-source. Collective Knowledge framework (CK) to unify benchmarking, optimization, and co-design of deep learning systems implementations and exchange results via a live multi-objective scoreboard. Systems evaluated under ReQuEST are diverse and include an FPGA-based accelerator, optimized deep learning libraries for x86 and ARM systems, and distributed inference in Amazon Cloud and over a cluster of Raspberry Pis. We finally discuss limitations to our approach, and how we plan improve upon those limitations for the upcoming SysML artifact evaluation effort.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Predicting computational reproducibility of data analysis pipelines in large population studies using collaborative filtering</title>
      <link>https://arxiv.org/abs/1809.10139</link>
      <pubDate>Wed, 03 Oct 2018 00:00:00 -0000</pubDate>
      <description>
        Evaluating the computational reproducibility of data analysis pipelines has become a critical issue. It is, however, a cumbersome process for analyses that involve data from large populations of subjects, due to their computational and storage requirements. We present a method to predict the computational reproducibility of data analysis pipelines in large population studies. We formulate the problem as a collaborative filtering process, with constraints on the construction of the training set. We propose 6 different strategies to build the training set, which we evaluate on 2 datasets, a synthetic one modeling a population with a growing number of subject types, and a real one obtained with neuroinformatics pipelines. Results show that one sampling method, "Random File Numbers (Uniform)" is able to predict computational reproducibility with a good accuracy. We also analyze the relevance of including file and subject biases in the collaborative filtering model. We conclude that the proposed method is able to speedup reproducibility evaluations substantially, with a reduced accuracy loss.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>A Serverless Tool for Platform Agnostic Computational Experiment Management</title>
      <link>https://arxiv.org/abs/1809.07693v1</link>
      <pubDate>Tue, 25 Sep 2018 00:00:00 -0000</pubDate>
      <description>
        Neuroscience has been carried into the domain of big data and high performance computing (HPC) on the backs of initiatives in data collection and an increasingly compute-intensive tools. While managing HPC experiments requires considerable technical acumen, platforms and standards have been developed to ease this burden on scientists. While web-portals make resources widely accessible, data organizations such as the Brain Imaging Data Structure and tool description languages such as Boutiques provide researchers with a foothold to tackle these problems using their own datasets, pipelines, and environments. While these standards lower the barrier to adoption of HPC and cloud systems for neuroscience applications, they still require the consolidation of disparate domain-specific knowledge. We present Clowdr, a lightweight tool to launch experiments on HPC systems and clouds, record rich execution records, and enable the accessible sharing of experimental summaries and results. Clowdr uniquely sits between web platforms and bare-metal applications for experiment management by preserving the flexibility of do-it-yourself solutions while providing a low barrier for developing, deploying and disseminating neuroscientific analysis.
      </description>

      <category>reproducible paper</category>

      <category>reproducibility infrastructure</category>

    </item>

    <item>
      <title>Exploration of reproducibility issues in scientometric research</title>
      <link>https://openaccess.leidenuniv.nl/bitstream/handle/1887/65315/STI2018_paper_113.pdf?sequence=1</link>
      <pubDate>Fri, 21 Sep 2018 00:00:00 -0000</pubDate>
      <description>
        In scientometrics, we have not yet had an intensive debate about the reproducibility of research published in our field, although concerns about  a  lack  of  reproducibility  have occasionally surfaced (see e.g. Glänzel &amp; Schöpflin 1994 and Van den Besselaar et al. 2017), and the need to improve the reproducibility is used as an important argument for open citation data  (see www.issi-society.org/open-citations-letter/).  We  initiated  a  first  discussion  about reproducibility  in  scientometrics  with  a  workshop  at  ISSI  2017  in  Wuhan. One of the outcomes was the sense that scientific fields differ with regard to the type and pervasiveness of  threats to the reproducibility  of  their  published  research,  last  but  not  least  due  to  their differences in modes of knowledge production, such as confirmatory versus exploratory study designs, and differences in methods and empirical objects.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Reproducibility and Replicability in a Fast-paced Methodological World</title>
      <link>https://psyarxiv.com/cnq4d/</link>
      <pubDate>Fri, 21 Sep 2018 00:00:00 -0000</pubDate>
      <description>
        Methodological developments and software implementations progress in increasingly faster time-frames. The introduction and widespread acceptance of pre-print archived reports and open-source software make state-of-the-art statistical methods readily accessible to researchers. At the same time, researchers more and more emphasize that their results should be reproducible (using the same data obtaining the same results), which is a basic requirement for assessing the replicability (obtaining similar results in new data) of results. While the age of fast-paced methodology greatly facilitates reproducibility, it also undermines it in ways not often realized by researchers. The goal of this paper is to make researchers aware of these caveats. I discuss sources of limited replicability and reproducibility in both the development of novel statistical methods and their implementation in software routines. Novel methodology comes with many researcher degrees of freedom, and new understanding comes with changing standards over time.  In software-development, reproducibility may be impacted due to software developing and changing over time, a problem that is greatly magnified by large dependency-trees between software-packages. The paper concludes with a list of recommendations for both developers and users of new methods to improve reproducibility of results.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Simple changes of individual studies can improve the reproducibility of the biomedical scientific process as a whole</title>
      <link>https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0202762</link>
      <pubDate>Tue, 11 Sep 2018 00:00:00 -0000</pubDate>
      <description>
        We developed a new probabilistic model to assess the impact of recommendations rectifying the reproducibility crisis (by publishing both positive and 'negative' results and increasing statistical power) on competing objectives, such as discovering causal relationships, avoiding publishing false positive results, and reducing resource consumption. In contrast to recent publications our model quantifies the impact of each single suggestion not only for an individual study but especially their relation and consequences for the overall scientific process. We can prove that higher-powered experiments can save resources in the overall research process without generating excess false positives. The better the quality of the pre-study information and its exploitation, the more likely this beneficial effect is to occur. Additionally, we quantify the adverse effects of both neglecting good practices in the design and conduct of hypotheses-based research, and the omission of the publication of 'negative' findings. Our contribution is a plea for adherence to or reinforcement of the good scientific practice and publication of 'negative' findings.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Issues in Reproducible Simulation Research</title>
      <link>https://link.springer.com/article/10.1007/s11538-018-0496-1</link>
      <pubDate>Tue, 11 Sep 2018 00:00:00 -0000</pubDate>
      <description>
        In recent years, serious concerns have arisen about reproducibility in science. Estimates of the cost of irreproducible preclinical studies range from 28 billion USD per year in the USA alone (Freedman et al. in PLoS Biol 13(6):e1002165, 2015) to over 200 billion USD per year worldwide (Chalmers and Glasziou in Lancet 374:86–89, 2009). The situation in the social sciences is not very different: Reproducibility in psychological research, for example, has been estimated to be below 50% as well (Open Science Collaboration in Science 349:6251, 2015). Less well studied is the issue of reproducibility of simulation research. A few replication studies of agent-based models, however, suggest the problem for computational modeling may be more severe than for laboratory experiments (Willensky and Rand in JASSS 10(4):2, 2007; Donkin et al. in Environ Model Softw 92:142–151, 2017; Bajracharya and Duboz in: Proceedings of the symposium on theory of modeling and simulation—DEVS integrative M&amp;S symposium, pp 6–11, 2013). In this perspective, we discuss problems of reproducibility in agent-based simulations of life and social science problems, drawing on best practices research in computer science and in wet-lab experiment design and execution to suggest some ways to improve simulation research practice.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Reproducibility study of a PDEVS model application to fire spreading</title>
      <link>https://dl.acm.org/citation.cfm?id=3275411</link>
      <pubDate>Tue, 11 Sep 2018 00:00:00 -0000</pubDate>
      <description>
        The results of a scientific experiment have to be reproduced to be valid. The scientific method is well known in experimental sciences but it is not always the case for computer scientists. Recent publications and studies has shown that there is a significant reproducibility crisis in Biology and Medicine. This problem has also been demonstrated for hundreds of publications in computer science where only a limited set of publication results could be reproduced. In this paper we present the reproducibility challenge and we examine the reproducibility of a Parallel Discrete Event System Specification (PDEVS) model with two different execution frameworks.
      </description>

      <category>reproducible paper</category>

      <category>reproducibility infrastructure</category>

    </item>

    <item>
      <title>Classification of Provenance Triples for Scientific Reproducibility: A Comparative Evaluation of Deep Learning Models in the ProvCaRe Project</title>
      <link>https://link.springer.com/chapter/10.1007/978-3-319-98379-0_3</link>
      <pubDate>Tue, 11 Sep 2018 00:00:00 -0000</pubDate>
      <description>
        Scientific reproducibility is key to the advancement of science as researchers can build on sound and validated results to design new research studies. However, recent studies in biomedical research have highlighted key challenges in scientific reproducibility as more than 70% of researchers in a survey of more than 1500 participants were not able to reproduce results from other groups and 50% of researchers were not able to reproduce their own experiments. Provenance metadata is a key component of scientific reproducibility and as part of the Provenance for Clinical and Health Research (ProvCaRe) project, we have: (1) identified and modeled important provenance terms associated with a biomedical research study in the S3 model (formalized in the ProvCaRe ontology); (2) developed a new natural language processing (NLP) workflow to identify and extract provenance metadata from published articles describing biomedical research studies; and (3) developed the ProvCaRe knowledge repository to enable users to query and explore provenance of research studies using the S3 model. However, a key challenge in this project is the automated classification of provenance metadata extracted by the NLP workflow according to the S3 model and its subsequent querying in the ProvCaRe knowledge repository. In this paper, we describe the development and comparative evaluation of deep learning techniques for multi-class classification of structured provenance metadata extracted from biomedical literature using 12 different categories of provenance terms represented in the S3 model. We describe the application of the Long Term Short Memory (LSTM) network, which has the highest classification accuracy of 86% in our evaluation, to classify more than 48 million provenance triples in the ProvCaRe knowledge repository (available at: https://provcare.case.edu/).
      </description>

      <category>reproducible paper</category>

      <category>reproducibility infrastructure</category>

    </item>

    <item>
      <title>The reproducibility opportunity</title>
      <link>https://www.nature.com/articles/s41562-018-0398-0</link>
      <pubDate>Tue, 04 Sep 2018 00:00:00 -0000</pubDate>
      <description>
        It is important for research users to know how likely it is that reported research findings are true. The Social Science Replication Project finds that, in highly powered experiments, only 13 of 21 high-profile reports could be replicated. Investigating the factors that contribute to reliable results offers new opportunities for the social sciences.
      </description>

      <category>news article</category>

    </item>

    <item>
      <title>Scientists Only Able to Reproduce Results for 13 out of 21 Human Behavior Studies</title>
      <link>https://gizmodo.com/scientists-only-able-to-reproduce-results-for-13-out-of-1828632222</link>
      <pubDate>Wed, 29 Aug 2018 00:00:00 -0000</pubDate>
      <description>
        If the results in a published study can’t be replicated in subsequent experiments, how can you trust what you read in scientific journals? One international group of researchers is well aware of this reproducibility crisis, and has been striving to hold scientists accountable. For their most recent test, they attempted to reproduce 21 studies from two of the top scientific journals, Science and Nature, that were published between 2010 and 2015. Only 13 of the reproductions produced the same results as the original study.
      </description>

      <category>news article</category>

    </item>

    <item>
      <title>Practical guidelines for rigor and reproducibility in preclinical and clinical studies on cardioprotection</title>
      <link>https://link.springer.com/article/10.1007/s00395-018-0696-8</link>
      <pubDate>Tue, 21 Aug 2018 00:00:00 -0000</pubDate>
      <description>
        We refer to the recent guidelines for experimental models of myocardial ischemia and infarction [279], and aim to provide now practical guidelines to ensure rigor and reproducibility in preclinical and clinical studies on cardioprotection. In line with the above guidelines [279], we define rigor as standardized state-of-the-art design, conduct and reporting of a study, which is then a prerequisite for reproducibility, i.e. replication of results by another laboratory when performing exactly the same experiment.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Editorial: Data repositories, registries, and standards in the search for valid and reproducible biomarkers</title>
      <link>https://onlinelibrary.wiley.com/doi/abs/10.1111/jcpp.12962</link>
      <pubDate>Tue, 21 Aug 2018 00:00:00 -0000</pubDate>
      <description>
        The paucity of major scientific breakthroughs leading to new or improved treatments, and the inability to identify valid and reproducible biomarkers that improve clinical management, has produced a crisis in confidence in the validity of our pathogenic theories and the reproducibility of our research findings. This crisis in turn has driven changes in standards for research methodologies and prompted calls for the creation of open‐access data repositories and the preregistration of research hypotheses. Although we should embrace the creation of repositories and registries, and the promise for greater statistical power, reproducibility, and generalizability of research findings they afford, we should also recognize that they alone are no substitute for sound design in minimizing study confounds, and they are no guarantor of faith in the validity of our pathogenic theories, findings, and biomarkers. One way, and maybe the only sure way, of knowing that we have a valid understanding of brain processes and disease mechanisms in human studies is by experimentally manipulating variables and predicting its effects on outcome measures and biomarkers.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>ReproServer: Making Reproducibility Easier and Less Intensive</title>
      <link>https://arxiv.org/abs/1808.01406</link>
      <pubDate>Sun, 12 Aug 2018 00:00:00 -0000</pubDate>
      <description>
        Reproducibility in the computational sciences has been stymied because of the complex and rapidly changing computational environments in which modern research takes place. While many will espouse reproducibility as a value, the challenge of making it happen (both for themselves and testing the reproducibility of others' work) often outweigh the benefits. There have been a few reproducibility solutions designed and implemented by the community. In particular, the authors are contributors to ReproZip, a tool to enable computational reproducibility by tracing and bundling together research in the environment in which it takes place (e.g. one's computer or server). In this white paper, we introduce a tool for unpacking ReproZip bundles in the cloud, ReproServer. ReproServer takes an uploaded ReproZip bundle (.rpz file) or a link to a ReproZip bundle, and users can then unpack them in the cloud via their browser, allowing them to reproduce colleagues' work without having to install anything locally. This will help lower the barrier to reproducing others' work, which will aid reviewers in verifying the claims made in papers and reusing previously published research.
      </description>

      <category>reproducible paper</category>

      <category>ReproZip</category>

    </item>

    <item>
      <title>Building research evidence towards reproducibility of animal research</title>
      <link>http://blogs.plos.org/everyone/2018/08/06/arrive-rct</link>
      <pubDate>Tue, 07 Aug 2018 00:00:00 -0000</pubDate>
      <description>
        Since our debut in late 2006, PLOS ONE has strived to promote best practices in research reporting as a way to improve reproducibility in research. We have supported initiatives towards increased transparency, as well as the gathering of evidence that can inform improvements in the quality of reporting in research articles. In line with this commitment, PLOS ONE collaborated in a randomized controlled trial (RCT) to test the impact of an intervention asking authors to complete a reporting checklist at the time of manuscript submission. The results from this trial have recently been posted on bioRxiv (1) and provide a further step toward building the necessary evidence base to inform editorial interventions towards improving reporting quality.
      </description>

      <category>popular news</category>

    </item>

    <item>
      <title>Open Science Badges in the Journal of Neurochemistry</title>
      <link>https://onlinelibrary.wiley.com/doi/full/10.1111/jnc.14536</link>
      <pubDate>Tue, 07 Aug 2018 00:00:00 -0000</pubDate>
      <description>
        The Open Science Framework (OSF) has the mission to increase openness, integrity, and reproducibility in research. The Journal of Neurochemistry became a signatory of their Transparency and Openness guidelines in 2016, which provides eight modular standards (Citation standards, Data Transparency, Analytic Methods/Code Transparency, Research Materials Transparency, Design and Analysis Transparency, Study Pre‐registration, Analysis Plan Transparency, Replication) with increasing levels of stringency. Furthermore, OSF recommends and offers a collection of practices intended to make scientific processes and results more transparent and available in a standardized way for reuse to people outside the research team. It includes making research materials, data, and laboratory procedures freely accessible online to anyone. This editorial announces the decision of the Journal of Neurochemistry to introduce Open Science Badges, maintained by the Open Science Badges Committee and by the Center for Open Science (COS). The Open Science Badges, visual icons placed on publications, certify that an open practice was followed and signal to readers that an author has shared the corresponding research evidence, thus, allowing an independent researcher to understand how to reproduce the procedure.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Assessment of the impact of shared brain imaging data on the scientific literature</title>
      <link>https://europepmc.org/abstract/med/30026557</link>
      <pubDate>Thu, 26 Jul 2018 00:00:00 -0000</pubDate>
      <description>
        Data sharing is increasingly recommended as a means of accelerating science by facilitating collaboration, transparency, and reproducibility. While few oppose data sharing philosophically, a range of barriers deter most researchers from implementing it in practice. To justify the significant effort required for sharing data, funding agencies, institutions, and investigators need clear evidence of benefit. Here, using the International Neuroimaging Data-sharing Initiative, we present a case study that provides direct evidence of the impact of open sharing on brain imaging data use and resulting peer-reviewed publications. We demonstrate that openly shared data can increase the scale of scientific studies conducted by data contributors, and can recruit scientists from a broader range of disciplines. These findings dispel the myth that scientific findings using shared data cannot be published in high-impact journals, suggest the transformative power of data sharing for accelerating science, and underscore the need for implementing data sharing universally.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>The State of Sustainable Research Software: Results from the Workshop on Sustainable Software for Science: Practice and Experiences (WSSSPE5.1)</title>
      <link>https://arxiv.org/pdf/1807.07387.pdf</link>
      <pubDate>Tue, 24 Jul 2018 00:00:00 -0000</pubDate>
      <description>
        This article summarizes motivations, organization, and activities of the Workshop on Sustainable Software for Science: Practice and Experiences (WSSSPE5.1) held in Manchester, UK in September 2017. The WSSSPE series promotes sustainable research software by positively impacting principles and best practices, careers, learning, and credit. This article discusses the Code of Conduct, idea papers, position papers, experience papers, demos, and lightning talks presented during the workshop. The main part of the article discusses the speed-blogging groups that formed during the meeting, along with the outputs of those sessions.
      </description>

      <category>reproducibility report</category>

    </item>

    <item>
      <title>The value of universally available raw NMR data for transparency, reproducibility, and integrity in natural product research†</title>
      <link>https://pubs.rsc.org/en/content/articlehtml/2018/np/c7np00064b</link>
      <pubDate>Thu, 19 Jul 2018 00:00:00 -0000</pubDate>
      <description>
        With contributions from the global natural product (NP) research community, and continuing the Raw Data Initiative, this review collects a comprehensive demonstration of the immense scientific value of disseminating raw nuclear magnetic resonance (NMR) data, independently of, and in parallel with, classical publishing outlets. A comprehensive compilation of historic to present-day cases as well as contemporary and future applications show that addressing the urgent need for a repository of publicly accessible raw NMR data has the potential to transform natural products (NPs) and associated fields of chemical and biomedical research. The call for advancing open sharing mechanisms for raw data is intended to enhance the transparency of experimental protocols, augment the reproducibility of reported outcomes, including biological studies, become a regular component of responsible research, and thereby enrich the integrity of NP research and related fields.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Preserving Workflow Reproducibility: The RePlay-DH Client as a Tool for Process Documentation</title>
      <link>http://www.lrec-conf.org/proceedings/lrec2018/pdf/707.pdf</link>
      <pubDate>Tue, 17 Jul 2018 00:00:00 -0000</pubDate>
      <description>
        In this  paper  we  present  a  software  tool  for  elicitation  and  management  of  process  metadata.   It  follows  our  previously  published design idea of an assistant for researchers that aims at minimizing the additional effort required for producing a sustainable workflow documentation.   With  the  ever-growing  number  of  linguistic  resources  available,  it  also  becomes  increasingly  important  to  provide proper  documentation  to  make  them  comparable  and  to  allow  meaningful  evaluations  for  specific  use  cases.   The  often  prevailing practice  of  post  hoc  documentation  of  resource  generation  or  research  processes  bears  the  risk  of  information  loss.   Not  only  does detailed documentation of a process aid in achieving reproducibility, it also increases usefulness of the documented work for others as a cornerstone of good scientific practice.  Time pressure together with the lack of simple documentation methods leads to workflow documentation in practice being an arduous and often neglected task. Our tool ensures a clean documentation for common workflows in natural language processing and digital humanities. Additionally, it can easily be integrated into existing institutional infrastructures.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Writing Empirical Articles: Transparency, Reproducibility, Clarity, and Memorability</title>
      <link>http://journals.sagepub.com/doi/abs/10.1177/2515245918754485</link>
      <pubDate>Tue, 17 Jul 2018 00:00:00 -0000</pubDate>
      <description>
        This article provides recommendations for writing empirical journal articles that enable transparency, reproducibility, clarity, and memorability. Recommendations for transparency include preregistering methods, hypotheses, and analyses; submitting registered reports; distinguishing confirmation from exploration; and showing your warts. Recommendations for reproducibility include documenting methods and results fully and cohesively, by taking advantage of open-science tools, and citing sources responsibly. Recommendations for clarity include writing short paragraphs, composed of short sentences; writing comprehensive abstracts; and seeking feedback from a naive audience. Recommendations for memorability include writing narratively; embracing the hourglass shape of empirical articles; beginning articles with a hook; and synthesizing, rather than Mad Libbing, previous literature.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>A Guide to Reproducibility in Preclinical Research</title>
      <link>https://europepmc.org/abstract/med/29995667</link>
      <pubDate>Tue, 17 Jul 2018 00:00:00 -0000</pubDate>
      <description>
        Many have raised concerns about the reproducibility of biomedical research. In this Perspective, the authors address this "reproducibility crisis" by distilling discussions around reproducibility into a simple guide to facilitate understanding of the topic.Reproducibility applies both within and across studies. The following questions address reproducibility within studies: "Within a study, if the investigator repeats the data management and analysis will she get an identical answer?" and "Within a study, if someone else starts with the same raw data, will she draw a similar conclusion?" Contrastingly, the following questions address reproducibility across studies: "If someone else tries to repeat an experiment as exactly as possible, will she draw a similar conclusion?" and "If someone else tries to perform a similar study, will she draw a similar conclusion?"Many elements of reproducibility from clinical trials can be applied to preclinical research (e.g., changing the culture of preclinical research to focus more on transparency and rigor). For investigators, steps toward improving reproducibility include specifying data analysis plans ahead of time to decrease selective reporting, more explicit data management and analysis protocols, and increasingly detailed experimental protocols, which allow others to repeat experiments. Additionally, senior investigators should take greater ownership of the details of their research (e.g., implementing active laboratory management practices, such as random audits of raw data [or at least reduced reliance on data summaries], more hands-on time overseeing experiments, and encouraging a healthy skepticism from all contributors). These actions will support a culture where rigor + transparency = reproducibility.This is an open-access article distributed under the terms of the Creative Commons Attribution-Non Commercial-No Derivatives License 4.0 (CCBY-NC-ND), where it is permissible to download and share the work provided it is properly cited. The work cannot be changed in any way or used commercially without permission from the journal.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Institutional Framework and Responsibilities: Facing Open Science’s challenges and assuring quality of research</title>
      <link>https://www.repository.cam.ac.uk/handle/1810/276203</link>
      <pubDate>Fri, 13 Jul 2018 00:00:00 -0000</pubDate>
      <description>
        This presentation to LERU workshop: Nurturing a Culture of Responsible Research in the Era of Open Science considered the issue of the credibility of science being in question in a 'post-truth' world and how reproducibility is adding to the problem. Open Science offers a solution, but it is not easy to implement, particularly by research institutions. The main issues relate to language used in the open space, that solutions look different to different disciplines, that researchers are often feeling "under siege" and that we need to reward good open practice.
      </description>

      <category>reproducibility talk</category>

    </item>

    <item>
      <title>Reproducible science: What, why, how</title>
      <link>https://digital.csic.es/handle/10261/145975</link>
      <pubDate>Fri, 13 Jul 2018 00:00:00 -0000</pubDate>
      <description>
        Most scientific papers are not reproducible: it is really hard, if not impossible, to understand how results are derived from data, and being able to regenerate them in the future (even by the same researchers). However, traceability and reproducibility of results are indispensable elements of highquality science, and an increasing requirement of many journals and funding sources. Reproducible studies include code able to regenerate results from the original data. This practice not only provides a perfect record of the whole analysis but also reduces the probability of errors and facilitates code reuse, thus accelerating scientific progress. But doing reproducible science also brings many benefits to the individual researcher, including saving time and effort, improved collaborations, and higher quality and impact of final publications. In this article we introduce reproducible science, why it is important, and how we can improve the reproducibility of our work. We introduce principles and tools for data management, analysis, version control, and software management that help us achieve reproducible workflows in the context of ecology.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>A Primer on the ‘Reproducibility Crisis’ and Ways to Fix It</title>
      <link>https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-8462.12262</link>
      <pubDate>Fri, 13 Jul 2018 00:00:00 -0000</pubDate>
      <description>
        This article uses the framework of Ioannidis (2005) to organise a discussion of issues related to the ‘reproducibility crisis’. It then goes on to use that framework to evaluate various proposals to fix the problem. Of particular interest is the ‘post‐study probability’, the probability that a reported research finding represents a true relationship. This probability is inherently unknowable. However, a number of insightful results emerge if we are willing to make some conjectures about reasonable parameter values. Among other things, this analysis demonstrates the important role that replication can play in improving the signal value of empirical research.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>A Statistical Model to Investigate the Reproducibility Rate Based on Replication Experiments</title>
      <link>https://onlinelibrary.wiley.com/doi/abs/10.1111/insr.12273</link>
      <pubDate>Fri, 13 Jul 2018 00:00:00 -0000</pubDate>
      <description>
        The reproducibility crisis, that is, the fact that many scientific results are difficult to replicate, pointing to their unreliability or falsehood, is a hot topic in the recent scientific literature, and statistical methodologies, testing procedures and p‐values, in particular, are at the centre of the debate. Assessment of the extent of the problem–the reproducibility rate or the false discovery rate–and the role of contributing factors are still an open problem. Replication experiments, that is, systematic replications of existing results, may offer relevant information on these issues. We propose a statistical model to deal with such information, in particular to estimate the reproducibility rate and the effect of some study characteristics on its reliability. We analyse data from a recent replication experiment in psychology finding a reproducibility rate broadly coherent with other assessments from the same experiment. Our results also confirm the expected role of some contributing factor (unexpectedness of the result and room for bias) while they suggest that the similarity between original study and the replica is not so relevant, thus mitigating some criticism directed to replication experiments.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Using Provenance for Generating Automatic Citations</title>
      <link>https://www.usenix.org/system/files/conference/tapp2018/tapp2018-paper-ton-that.pdf</link>
      <pubDate>Thu, 12 Jul 2018 00:00:00 -0000</pubDate>
      <description>
        When computational experiments include only datasets, they could be shared through the Uniform Resource Identifiers (URIs) or Digital Object Identifiers (DOIs) which point to these resources. However, experiments seldom include only datasets, but most often also include software, execution results, provenance, and other associated documentation. The Research Object has recently emerged as a comprehensive and systematic method for aggregation and identification of diverse elements of computational experiments. While an entire Research Object may be citable using a URI or a DOI, it is often desirable to cite specific sub-components of a research object to help identify, authorize, date, and retrieve the published sub-components of these objects. In this paper, we present an approach to automatically generate citations for sub-components of research objects by using the object’s recorded provenance traces. The generated citations can be used "as is" or taken as suggestions that can be grouped and combined to produce higher level citations.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Variable Bibliographic Database Access Could Limit Reproducibility</title>
      <link>https://academic.oup.com/bioscience/advance-article/doi/10.1093/biosci/biy074/5050469</link>
      <pubDate>Thu, 12 Jul 2018 00:00:00 -0000</pubDate>
      <description>
        Bibliographic databases provide access to scientific literature through targeted queries. The most common uses of these services, aside from accessing scientific literature for personal use, are to find relevant citations for formal surveys of scientific literature, such as systematic reviews or meta-analysis, or to estimate the number of publications on a certain topic as a measure of sampling effort. Bibliographic search tools vary in the level of access to the scientific literature they allow. For instance, Google Scholar is a bibliographic search engine which allows users to find (but not necessarily access) scientific literature for no charge, whereas other services, such as Web of Science, are subscription based, allowing access to full texts of academic works at costs that can exceed $100,000 annually for large universities (Goodman 2005). One of the most commonly used bibliographic databases, Clarivate Analytics–produced Web of Science, offers tailored subscriptions to their citation indexing service. This flexibility allows subscriptions and resulting access to be tailored to the needs of researchers at the institution (Goodwin 2014). However, there are issues created by this differential access, which we discuss further below.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Software Reproducibility: How to put it into practice?</title>
      <link>https://osf.io/z48cm/</link>
      <pubDate>Wed, 11 Jul 2018 00:00:00 -0000</pubDate>
      <description>
        On 24 May 2018, Maria Cruz, Shalini Kurapati, and Yasemin Türkyilmaz-van der Velden led a workshop titled “Software Reproducibility: How to put it into practice?”, as part of the event Towards cultural change in data management - data stewardship in practice held at TU Delft, the Netherlands. There were 17 workshop participants, including researchers, data stewards, and research software engineers. Here we describe the rationale of the workshop, what happened on the day, key discussions and insights, and suggested next steps.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>DataPackageR: Reproducible data preprocessing, standardization and sharing using R/Bioconductor for collaborative data analysis</title>
      <link>https://gatesopenresearch.org/articles/2-31/v1</link>
      <pubDate>Thu, 05 Jul 2018 00:00:00 -0000</pubDate>
      <description>
        A central tenet of reproducible research is that scientific results are published along with the underlying data and software code necessary to reproduce and verify the findings. A host of tools and software have been released that facilitate such work-flows and scientific journals have increasingly demanded that code and primary data be made available with publications. There has been little practical advice on implementing reproducible research work-flows for large ’omics’ or systems biology data sets used by teams of analysts working in collaboration. In such instances it is important to ensure all analysts use the same version of a data set for their analyses. Yet, instantiating relational databases and standard operating procedures can be unwieldy, with high "startup" costs and poor adherence to procedures when they deviate substantially from an analyst’s usual work-flow. Ideally a reproducible research work-flow should fit naturally into an individual’s existing work-flow, with minimal disruption. Here, we provide an overview of how we have leveraged popular open source tools, including Bioconductor, Rmarkdown, git version control, R, and specifically R’s package system combined with a new tool DataPackageR, to implement a lightweight reproducible research work-flow for preprocessing large data sets, suitable for sharing among small-to-medium sized teams of computational scientists. Our primary contribution is the DataPackageR tool, which decouples time-consuming data processing from data analysis while leaving a traceable record of how raw data is processed into analysis-ready data sets. The software ensures packaged data objects are properly documented and performs checksum verification of these along with basic package version management, and importantly, leaves a record of data processing code in the form of package vignettes. Our group has implemented this work-flow to manage, analyze and report on pre-clinical immunological trial data from multi-center, multi-assay studies for the past three years.
      </description>

      <category>reproducible paper</category>

      <category>reproducibility infrastructure</category>

    </item>

    <item>
      <title>How to Read a Research Compendium</title>
      <link>https://arxiv.org/pdf/1806.09525.pdf</link>
      <pubDate>Sat, 30 Jun 2018 00:00:00 -0000</pubDate>
      <description>
        Researchers spend a great deal of time reading research papers. Keshav (2012) provides a three-pass method to researchers to improve their reading skills. This article extends Keshav's method for reading a research compendium. Research compendia are an increasingly used form of publication, which packages not only the research paper's text and figures, but also all data and software for better reproducibility. We introduce the existing conventions for research compendia and suggest how to utilise their shared properties in a structured reading process. Unlike the original, this article is not build upon a long history but intends to provide guidance at the outset of an emerging practice.
      </description>

      <category>reproducible paper</category>

      <category>reproducibility infrastructure</category>

    </item>

    <item>
      <title>Teaching computational reproducibility for neuroimaging</title>
      <link>https://arxiv.org/pdf/1806.06145.pdf</link>
      <pubDate>Sat, 23 Jun 2018 00:00:00 -0000</pubDate>
      <description>
        We describe a project-based introduction to reproducible and collaborative neuroimaging analysis. Traditional teaching on neuroimaging usually consists of a series of lectures that emphasize the big picture rather than the foundations on which the techniques are based. The lectures are often paired with practical workshops in which students run imaging analyses using the graphical interface of specific neu-roimaging software packages. Our experience suggests that this combination leaves the student with asuperficial understanding of the underlying ideas, and an informal, inefficient, and inaccurate approach to  analysis.   To  address  these  problems,  we  based  our  course  around a substantial  open-ended  group project.  This allowed us to teach:  (a) computational tools to ensure computationally reproducible work,such as the Unix command line, structured code, version control, automated testing, and code reviewand (b) a clear understanding of the statistical techniques used for a basic analysis of a single run in an MRI scanner.  The emphasis we put on the group project showed the importance of standard computational tools for accuracy, efficiency, and collaboration.  The projects were broadly successful in engagingstudents in working reproducibly on real scientific questions.  We propose that a course on this modelshould be the foundation for future programs in neuroimaging.  We believe it will also serve as a modelfor teaching efficient and reproducible research in other fields of computational science
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>YAMP: a containerised workflow enabling reproducibility in metagenomics research</title>
      <link>https://watermark.silverchair.com/giy072.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAdIwggHOBgkqhkiG9w0BBwagggG_MIIBuwIBADCCAbQGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQM9e5qZs5DsbcT7x6rAgEQgIIBhSLDG-1fl8D5GYdRDo-2wBE35B-JUWT1SELphgsGS_7CBSCg9GQc7B81NfOMex-RSvfwHbIBIbz5nksrXpTSpkpqjAMuzfrTNxDmLykrRHKT77Aor9BJNfACV3E2eOy1GXT6B08kbo77o85nn8G8vdSE9Qf7Dv-ACvv2bEi07bZrQ2WPC14oEFIOWKmorKXrhIcQrI7CrU3MyoypLWGhEsLh4BnTgSbs13V4yTpI7FbPur6wUMVBP81cru_Ud33rwrH4GKKADD8RYRMMwuN3RU_ZSl-XVDe2ph6RBw6KPfnA_imkXp8SRjPfsy5xieC8JJo4RqnQKsrMp895HOc3OI5nD0gyPJqWpPqqBOdDGb5hPaFpBfT_bJidP6xsIrQP9zleZAHSsEewOPfDOzCnycVPoJIhcyWCpTu3McLAu3IRny3XZKa2EtxvgRR9Tgcm7s-WHYh1MxvlabnKaboJE08SiPCr_T6CCKEkUhTovpwja1NrmhuQM5HYPY9ZRjrPd4-Nt2xG</link>
      <pubDate>Tue, 19 Jun 2018 00:00:00 -0000</pubDate>
      <description>
        YAMP is a user-friendly workflow that enables the analysis of whole shotgun metagenomic data while using containerisation to ensure computational reproducibility and facilitate collaborative research. YAMP can be executed on any UNIX-like system, and offers seamless support for multiple job schedulers as well as for Amazon AWS cloud. Although YAMP has been developed to be ready-to-use by non-experts, bioinformaticians will appreciate its flexibility, modularisation, and simple customisation. The YAMP script, parameters, and documentation are available at https://github.com/alesssia/YAMP.
      </description>

      <category>reproducible paper</category>

      <category>reproducibility infrastructure</category>

    </item>

    <item>
      <title>The RAMP framework: from reproducibility to transparency in the design and optimization of scientific workflows</title>
      <link>https://openreview.net/pdf?id=Syg4NHz4eQ</link>
      <pubDate>Tue, 19 Jun 2018 00:00:00 -0000</pubDate>
      <description>
        The RAMP (Rapid Analytics and Model Prototyping) is a software and project management tool developed by the Paris-Saclay Center for Data Science. The original goal was to accelerate the adoption of high-quality data science solutions for domain science problems by running rapid collaborative prototyping sessions. Today it is a full-blown data science project management tool promoting reproducibility, fair and transparent model evaluation, and democratization of data science.  We have used the framework for setting up and solving about twenty scientific problems, for organizing scientific sub-communities around these events, and for training novice data scientists.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Three Dimensions of Reproducibility in Natural Language Processing</title>
      <link>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5998676/</link>
      <pubDate>Tue, 19 Jun 2018 00:00:00 -0000</pubDate>
      <description>
        Despite considerable recent attention to problems with reproducibility of scientific research, there is a striking lack of agreement about the definition of the term. That is a problem, because the lack of a consensus definition makes it difficult to compare studies of reproducibility, and thus to have even a broad overview of the state of the issue in natural language processing. This paper proposes an ontology of reproducibility in that field. Its goal is to enhance both future research and communication about the topic, and retrospective meta-analyses. We show that three dimensions of reproducibility, corresponding to three kinds of claims in natural language processing papers, can account for a variety of types of research reports. These dimensions are reproducibility of a conclusion, of a finding, and of a value. Three biomedical natural language processing papers by the authors of this paper are analyzed with respect to these dimensions.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Enabling the Verification of Computational Results: An Empirical Evaluation of Computational Reproducibility</title>
      <link>https://dl.acm.org/citation.cfm?doid=3214239.3214242</link>
      <pubDate>Sat, 16 Jun 2018 00:00:00 -0000</pubDate>
      <description>
        The ability to independently regenerate published computational claims is widely recognized as a key component of scientific reproducibility. In this article we take a narrow interpretation of this goal, and attempt to regenerate published claims from author-supplied information, including data, code, inputs, and other provided specifications, on a different computational system than that used by the original authors. We are motivated by Claerbout and Donoho's exhortation of the importance of providing complete information for reproducibility of the published claim. We chose the Elsevier journal, the Journal of Computational Physics, which has stated author guidelines that encourage the availability of computational digital artifacts that support scholarly findings. In an IRB approved study at the University of Illinois at Urbana-Champaign (IRB #17329) we gathered artifacts from a sample of authors who published in this journal in 2016 and 2017. We then used the ICERM criteria generated at the 2012 ICERM workshop "Reproducibility in Computational and Experimental Mathematics" to evaluate the sufficiency of the information provided in the publications and the ease with which the digital artifacts afforded computational reproducibility. We find that, for the articles for which we obtained computational artifacts, we could not easily regenerate the findings for 67% of them, and we were unable to easily regenerate all the findings for any of the articles. We then evaluated the artifacts we did obtain (55 of 306 articles) and find that the main barriers to computational reproducibility are inadequate documentation of code, data, and workflow information (70.9%), missing code function and setting information, and missing licensing information (75%). We recommend improvements based on these findings, including the deposit of supporting digital artifacts for reproducibility as a condition of publication, and verification of computational findings via re-execution of the code when possible.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Improving Reproducibility of Distributed Computational Experiments</title>
      <link>https://dl.acm.org/citation.cfm?doid=3214239.3214241</link>
      <pubDate>Sat, 16 Jun 2018 00:00:00 -0000</pubDate>
      <description>
        Conference and journal publications increasingly require experiments associated with a submitted article to be repeatable. Authors comply to this requirement by sharing all associated digital artifacts, i.e., code, data, and environment configuration scripts. To ease aggregation of the digital artifacts, several tools have recently emerged that automate the aggregation of digital artifacts by auditing an experiment execution and building a portable container of code, data, and environment. However, current tools only package non-distributed computational experiments. Distributed computational experiments must either be packaged manually or supplemented with sufficient documentation. In this paper, we outline the reproducibility requirements of distributed experiments using a distributed computational science experiment involving use of message-passing interface (MPI), and propose a general method for auditing and repeating distributed experiments. Using Sciunit we show how this method can be implemented. We validate our method with initial experiments showing application re-execution runtime can be improved by 63% with a trade-off of longer run-time on initial audit execution.
      </description>

      <category>reproducible paper</category>

    </item>

    <item>
      <title>Popper Pitfalls</title>
      <link>https://dl.acm.org/citation.cfm?doid=3214239.3214243</link>
      <pubDate>Sat, 16 Jun 2018 00:00:00 -0000</pubDate>
      <description>
        We describe the four publications we have tried to make reproducible and discuss how each paper has changed our workflows, practices, and collaboration policies. The fundamental insight is that paper artifacts must be made reproducible from the start of the project; artifacts are too difficult to make reproducible when the papers are (1) already published and (2) authored by researchers that are not thinking about reproducibility. In this paper, we present the best practices adopted by our research laboratory, which was sculpted by the pitfalls we have identified for the Popper convention. We conclude with a "call-to-arms" for the community focused on enhancing reproducibility initiatives for academic conferences, industry environments, and national laboratories. We hope that our experiences will shape a best practices guide for future reproducible papers.
      </description>

      <category>reproducible paper</category>

    </item>

  </channel>
</rss>